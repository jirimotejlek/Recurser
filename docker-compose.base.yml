services:
  chromadb:
    image: chromadb/chroma:1.0.15
    container_name: chromadb
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # ChromaDB settings
      IS_PERSISTENT: "TRUE"
      PERSIST_DIRECTORY: "/chroma/chroma"
      # Explicitly set the host and port
      CHROMA_SERVER_HOST: "0.0.0.0"
      CHROMA_SERVER_HTTP_PORT: "8000"
    volumes:
      - chroma_data:/chroma/chroma
    networks: [backend]
<<<<<<< HEAD:docker-compose.yml

  # llm:
  #   build: ./ollama
  #   container_name: ollama-llm
  #   restart: unless-stopped
  #   ports:
  #     - "11434:11434"
  #   environment:
  #     # Ollama settings
  #     OLLAMA_HOST: "0.0.0.0"
  #     OLLAMA_MODELS: "/root/.ollama/models"
  #     # Model to preload
  #     OLLAMA_MODEL: "gemma3n:e2b"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks: [backend]
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

=======
  
>>>>>>> 77fa0da946314f443835eda2839689281d280be1:docker-compose.base.yml
  recurser-ui:
    build: ./recurser_ui
    container_name: recurser-ui
    command: streamlit run /app/recurser_ui.py
    ports: ["8501:8501"]
    volumes:
      - ./recurser_ui:/app
      # for development
      - ./recurser_ui/.streamlit/config.dev.toml:/app/.streamlit/config.toml
      # for deployment
      #- ./recurser_ui/.streamlit/config.prod.toml:/app/.streamlit/config.toml
    environment:
      # ChromaDB connection settings
      CHROMA_HOST: chromadb
      CHROMA_PORT: 8000
      # Optimizer
      OPTIMIZER_HOST: optimizer
      OPTIMIZER_PORT: 5050
<<<<<<< HEAD:docker-compose.yml
      # llm dispatcher
      LLM_DISPATCHER_HOST: api
      LLM_DISPATCHER_PORT: 5100
      #LLMUSE: OpenAI
      LLMUSE: localmini
      OPTIMIZER_KEY: "your-secret-api-key-change-this"      
=======
      # LLM Dispatcher
      LLM_DISPATCHER: llm-dispatcher
      LLM_DISPATCHER_PORT: 5100
>>>>>>> 77fa0da946314f443835eda2839689281d280be1:docker-compose.base.yml
      # Streamlit settings
      STREAMLIT_WATCH_USE_POLLING: "true"
      STREAMLIT_DEV_FRONTEND: "false"
      # Python path for imports
      PYTHONPATH: /app
    networks: [backend]
  
  optimizer:
    build: ./optimizer
    container_name: optimizer
    restart: unless-stopped
    ports:
      - "5050:5050"
    environment:
      # Flask settings
      FLASK_ENV: development
      FLASK_DEBUG: "true"
      # ChromaDB connection
      CHROMA_HOST: chromadb
      CHROMA_PORT: 8000
    volumes:
      - ./optimizer:/app
    networks: [backend]

  search-engine:
    build: ./search_engine
    container_name: search-engine
    restart: unless-stopped
    ports:
      - "5150:5150"
    environment:
      # Flask settings
      FLASK_ENV: development
      FLASK_DEBUG: "true"
    volumes:
      - ./search_engine:/app
    networks: [backend]
  
  llm-dispatcher:
    build:
      context: ./llm_dispatcher
      args:
        LLM_PROVIDER: ${LLM_PROVIDER:-local}
    container_name: llm-dispatcher
    restart: unless-stopped
    ports:
      - "5100:5100"
    environment:
      # Flask settings
      FLASK_ENV: development
      FLASK_DEBUG: "true"
      # ChromaDB connection
      CHROMA_HOST: chromadb
      CHROMA_PORT: 8000
    volumes:
      - ./llm_dispatcher:/app
    networks: [backend]    
   
volumes:
  chroma_data:
  ollama_data:

networks:
  backend: